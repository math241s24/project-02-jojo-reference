[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Project 2",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Project 2",
    "section": "Dataset",
    "text": "Dataset\n\n## read data"
  },
  {
    "objectID": "proposal.html#reason-for-choosing-this-dataset",
    "href": "proposal.html#reason-for-choosing-this-dataset",
    "title": "Project 2",
    "section": "Reason for Choosing this Dataset",
    "text": "Reason for Choosing this Dataset"
  },
  {
    "objectID": "proposal.html#questions-and-analysis-plan",
    "href": "proposal.html#questions-and-analysis-plan",
    "title": "Project 2",
    "section": "Questions and Analysis Plan",
    "text": "Questions and Analysis Plan"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Your project title",
    "section": "",
    "text": "This project was developed by XX. For MATH/STAT241 at Reed College. The team is comprised of the following team members."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "project-02",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "jojo_reference_presentation.html",
    "href": "jojo_reference_presentation.html",
    "title": "Jojo_reference_presentation",
    "section": "",
    "text": "# install.packages(\"RSelenium\")\n#install.packages(\"selenider\")\n#install.packages(\"chromote\")\n#install.packages(\"Rcrawler\")\n#install.packages(\"tidytext\")\n#install.packages(\"purrr\")\n#install.packages(\"glue\")\n\n\nlibrary(tidyverse)\nlibrary(Rcrawler)\nlibrary(tidytext)\nlibrary(chromote)\nlibrary(stringr)\nlibrary(rvest)\nlibrary(readr)\nlibrary(purrr)\nlibrary(httr)\nlibrary(glue)\n\n\n# Define URLs for all episodes\nurls &lt;- c(\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96633\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96634\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96635\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96636\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96637\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96638\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96639\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96640\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96641\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96642\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96643\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96644\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96645\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96646\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96647\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96648\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96649\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96650\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96651\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96652\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96653\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96654\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96655\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96656\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96657\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96658\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96659\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96660\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96661\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96662\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96663\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96664\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96665\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96666\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96667\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96668\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96669\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96670\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96671\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96672\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96673\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96674\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96675\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96676\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96677\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96678\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96679\",\n  \"https://transcripts.foreverdreaming.org/viewtopic.php?t=96680\"\n)\n\n# Define a function to scrape episode text\nscrape_episode &lt;- function(url) {\n  # Read the HTML content from the webpage\n  episode_html &lt;- read_html(url)\n  \n  # Extract the text content of the \"page-body\" element\n  episode_text &lt;- html_text(html_nodes(episode_html, \".postbody\"))\n  \n # episode_text &lt;- episode_text%&gt;%\n    #substr(episode_text, start = 907, stop = nchar(episode_text)) NOT NEEDED I THINK, SEEMS TO SCREW WITH IT\n  \n  # Remove any HTML tags and extra whitespace\n  cleaned_episode &lt;- gsub(\"&lt;.*?&gt;\", \"\", episode_text)\n  cleaned_episode &lt;- trimws(cleaned_episode)\n  \n  # Remove \"\\n\", \"\\n-\", and \"\\t\" occurrences\n  cleaned_episode &lt;- gsub(\"\\n\", \"\", cleaned_episode)\n  cleaned_episode &lt;- gsub(\"\\n-\", \"\", cleaned_episode)\n  cleaned_episode &lt;- gsub(\"\\t\", \"\", cleaned_episode)\n  \n  # Remove any extra whitespace\n  cleaned_episode &lt;- trimws(cleaned_episode)\n  \n  return(cleaned_episode)\n}\n\n# Define a list to store cleaned episode texts\nall_episodes &lt;- list()\n\n# Scrape text for each episode\nfor (i in 1:length(urls)) {\n  episode_text &lt;- scrape_episode(urls[i])\n  all_episodes[[i]] &lt;- episode_text\n}\n\n# Now you have a list where each element contains the cleaned text for each episode\n\nTurning all the episodes into a dataframe and remove stop words\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\ndata(\"stop_words\")\n\n# Combine all episodes into a single data frame\nall_episodes_df &lt;- map_dfr(all_episodes, ~ tibble(text = .x), .id = \"episode\")\n\n# Tokenize the text\nall_episodes_words &lt;- all_episodes_df %&gt;%\n  unnest_tokens(output = word, input = text) \n\n#TRY TO REMOVE STOP WORDS\nall_episodes_words &lt;- all_episodes_words %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\ntokenize by word and plot most common words\n\n# Count the occurrences of each word\nword_counts &lt;- all_episodes_words %&gt;%\n  count(word, sort = TRUE)\n\n# Filter words with counts greater than 1000\nword_counts_filtered &lt;- word_counts %&gt;%\n  filter(n &gt; 100)\n\n# Plot the word frequencies\nword_counts_filtered %&gt;%\n  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +\n  geom_col() +\n  guides(fill = FALSE) +\n  labs(y = \"Word\", x = \"Frequency\") +\n  theme_minimal() +\n  ggtitle(\"Most Frequent Words Across All Episodes (Occurrences &gt; 100)\")\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\nSENTIMENT ANALYSIS\n\nall_episodes_words %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %&gt;%\n  count(sentiment, word, sort = TRUE) %&gt;%\n  group_by(sentiment) %&gt;%\n  slice_head(n = 20) %&gt;%\n  ggplot(aes(y = fct_reorder(word, n), x = n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free\") +\n  labs(\n    title = \"Sentiment and frequency of words across Stardust Crusaders\",\n    subtitle = \"Bing lexicon\",\n    y = NULL, x = NULL\n  ) +\n  scale_fill_manual(values = c(\"positive\" = \"lightblue\", \"negative\" = \"lightcoral\"))\n\n\n\n\nSENTIMENT BY EPISODE\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Assuming all_episodes is a list of episode texts\n\n# Combine all episodes into a single data frame\nall_episodes_df &lt;- map_dfr(all_episodes, ~ tibble(text = .x), .id = \"episode\")\n\n# Tokenize the text\nall_episodes_words &lt;- all_episodes_df %&gt;%\n  unnest_tokens(output = word, input = text) \n\n# Get sentiments of words using the \"bing\" lexicon\nall_episodes_sentiments &lt;- all_episodes_words %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %&gt;%\n  select(episode, sentiment)\n\n# Count the occurrences of each sentiment by episode\nsentiment_counts &lt;- all_episodes_sentiments %&gt;%\n  count(episode, sentiment)\n\n# Plot the sentiment counts by episode\nggplot(sentiment_counts, aes(x = episode, y = n, fill = sentiment)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Sentiment by Episode\",\n       x = \"Episode\",\n       y = \"Count\",\n       fill = \"Sentiment\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n#it seems to vary somewhat regularly. does this indicate that there is a regular cycle in mood built into the pacing of the show??\n\nLOOKING AT BIGRAM FREQUENCY\n\n# Load the dplyr package\nlibrary(dplyr)\n\nthreshold &lt;- 75\n\n# Tokenize the text\nall_episodes_bigrams &lt;- all_episodes_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  mutate(i = row_number()) %&gt;%    # add index for later grouping\n  unnest_tokens(word, bigram, drop = FALSE) %&gt;%    # tokenize bigrams into words\n  anti_join(stop_words) %&gt;%    # drop rows with stop words\n  group_by(i) %&gt;%    # group by bigram index\n  filter(n() == 2) %&gt;%    # drop bigram instances where only one word left\n  summarise(bigram = unique(bigram), .groups = \"drop\")\n\nJoining with `by = join_by(word)`\n\nall_episodes_bigrams %&gt;%\n  mutate(\n    bigram = if_else(bigram == \"care home\", \"care home(s)\", bigram),\n    bigram = if_else(bigram == \"care homes\", \"care home(s)\", bigram)\n  ) %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  filter(n &gt; threshold) %&gt;%\n  ggplot(aes(y = fct_reorder(bigram, n), x = n, fill = n)) +\n  geom_col() +\n  guides(fill = FALSE) +\n  labs(\n    title = \"Frequency of bigrams Stardust Crusaders\",\n    subtitle = glue(\"Bigrams occurring more than {threshold} times\"),\n    y = NULL, x = NULL\n  )"
  }
]